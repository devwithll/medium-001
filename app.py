import os
import uvicorn
from dotenv import load_dotenv
from typing import List, TypedDict, Annotated
from contextlib import asynccontextmanager

# FastAPI Imports
# ---------------
from fastapi import FastAPI, Request, Form
from fastapi.responses import HTMLResponse, RedirectResponse
from fastapi.templating import Jinja2Templates
from starlette import status
# ---------------

# Langchain/Langgraph Imports
# ---------------------------
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
from langgraph.graph import StateGraph, END
# ---------------------------

# Configuration
# -------------
load_dotenv()
SQLITE_DATABASE_PATH = os.getenv("SQLITE_DATABASE_PATH")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# Identifier for the user's chat thread
SESSION_ID = 1 
# -------------

# LangGraph State Definition
class ChatState(TypedDict):
    user_prompt: str
    messages: Annotated[List[BaseMessage], add_messages]
    response: str


# Graph Nodes
# -----------

# Define an asynchronous function to process user input
async def process_user_prompt_node(state: ChatState):
  # Extract the user prompt from the 'state' dictionary
  user_prompt = state['user_prompt']

  # Return a dictionary with a list of a HumanMessage object
  # The HumanMessage contains the user prompt
  return {"messages": [HumanMessage(content=user_prompt)]}

# Define an asynchronous function that sends the user prompt to the LLM and gets a response
async def call_model_node(state: ChatState):
  # Create a ChatOpenAI instance using the GPT-4o-mini model
  llm = ChatOpenAI(model="gpt-4o-mini", api_key=OPENAI_API_KEY)

  # Retrieve the list of messages (conversation history) from the 'state'
  messages = state['messages']

  # Check if there are no messages or the last message is not from the user  
  if not messages or not isinstance(messages[-1], HumanMessage):
      # Return a default response
      return {"response": "Hmm, I didn't get a message from you. Could you please repeat?"}
  try:
      # Call the model asynchronously with the current messages
      response = await llm.ainvoke(messages)

      # Return the LLM response
      return {"response": response.content}
  except Exception as e:
      # Return a fallback error message
      return {"response": "Sorry, I encountered an error trying to respond."}

# Define an asynchronous function to process the LLM response
async def process_bot_response_node(state: ChatState):
    # Extract the response generated by the LLM from the 'state'
    bot_response = state['response']

    # Return a dictionary with a list containing an AIMessage object
    # The AIMessage contains the LLM response
    return {"messages": [AIMessage(content = bot_response)]}

# -----------

# FastAPI Lifespan Function
@asynccontextmanager
async def lifespan(app: FastAPI):
  
    # Start up Logic - Below code runs when the app starts
    print("Starting up: Initializing resources")

    # Initiate the AsyncSqliteSaver Checkpointer for memory persistence.
    # It creates an asynchronous SQLite database connection.
    async with AsyncSqliteSaver.from_conn_string(SQLITE_DATABASE_PATH) as checkpointer:
        print("AsyncSqliteSaver connection established.")

        # Initiate the Graph
        workflow = StateGraph(ChatState)

        # Add Nodes
        workflow.add_node("process_user_prompt", process_user_prompt_node)
        workflow.add_node("call_model", call_model_node)
        workflow.add_node("generate_response", process_bot_response_node)

        # Add Edges
        workflow.set_entry_point("process_user_prompt")
        workflow.add_edge("process_user_prompt", "call_model")
        workflow.add_edge("call_model", "generate_response")
        workflow.add_edge("generate_response", END)

        # Compile the graph with the checkpointer instance
        # Store the compiled graph in Application State for easy access
        app.state.graph = workflow.compile(checkpointer=checkpointer)
        print("LangGraph graph compiled with AsyncSqliteSaver")

        # Store checkpointer in Application State
        app.state.checkpointer = checkpointer

        # Shutdown Logic - Below code runs when the app shutdowns
        yield
    
        # checkpointer connection pool is closed automatically when 'async with' exits
        print("Application shutdown: Resources released by context manager.")


# FastAPI App Initialization with Lifespan
# ----------------------------------------
app = FastAPI(lifespan = lifespan)

## Mount Templates Directory
templates = Jinja2Templates(directory = "templates")
# ----------------------------------------

# Get Conversation History
async def get_chat_history_messages(request: Request, thread_id: str):

    # Create a configuration dictionary containing the thread ID
    # It will be used to look up the correct chat session
    config = {"configurable": {"thread_id": thread_id}}

    # Initialize an empty list to store chat messages
    chat_history_messages = []

    # Access Compiled Graph from Application State
    app_graph = request.app.state.graph

    try:
        # Get the current state of the chat session
        current_state = await app_graph.aget_state(config)

        # Check if current state is valid
        if current_state and current_state.values.get("messages"):
            # Store messages in chat_history_messages list
            chat_history_messages = current_state.values["messages"]
            print(f"Loaded {len(chat_history_messages)} messages for session {thread_id}")
        else:
            print(f"No existing state found for session {thread_id}")
    except Exception as e:
        print(f"Error retrieving state for session {thread_id} from SQLite: {e}")
    
    # Return conversation history
    return chat_history_messages


# FastAPI Endpoints
# -----------------

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    """Render Chat Page with Conversation History"""

    # Get the chat history for the current session
    chat_history = await get_chat_history_messages(request, SESSION_ID)

    # Render the "index.html" template and pass it the request and chat history
    return templates.TemplateResponse("index.html", {
        "request": request,
        "chat_history": chat_history
    })

@app.post("/chat")
async def chat_endpoint(request: Request, user_prompt: str = Form(...)):
    """Handle user input, run the graph, and redirect."""

    # Check if user submitted an empty message
    if not user_prompt:
        # Redirect back to the homepage if input is empty
        return RedirectResponse(url="/", status_code = status.HTTP_303_SEE_OTHER)

    # Create an input dictionary with the user's prompt for the graph
    graph_input = {"user_prompt": user_prompt}

    # Create a configuration dictionary with the current session/thread ID
    config = {"configurable": {"thread_id": SESSION_ID}}

    # Access the compiled graph from app state
    app_graph = request.app.state.graph

    try:
        print("START - Invoke Compiled Graph")
        # Trigger the graph with user input and session config
        await app_graph.ainvoke(graph_input, config=config)
        print("END - Invoke Compiled Graph")

    except Exception as e:
        print(f"Error invoking graph for session {SESSION_ID}: {e}")

    # Redirect the user back to the homepage
    return RedirectResponse(url="/", status_code = status.HTTP_303_SEE_OTHER)

# -----------------

if __name__ == "__main__":
    uvicorn.run("app:app", host="0.0.0.0", port=8000, reload=False)
